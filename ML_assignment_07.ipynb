{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "What is the definition of a target function? In the sense of a real-life example, express the target function. How is a target function's fitness assessed?\n"
      ],
      "metadata": {
        "id": "yzAEdsQ6Tm5L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Vqnvbb5ZTiWl"
      },
      "outputs": [],
      "source": [
        "#A target function is a mathematical function that measures how well a solution or model performs a specific task or achieves a goal.\n",
        "#example\n",
        "\n",
        "def target_function(x):\n",
        "  return -(x ** 2)\n",
        "\n",
        "def assess_fitness(solution):\n",
        "  return target_function(solution)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_value = 5\n",
        "result = target_function(x_value)\n",
        "print(\"Target Function result:\", result)\n",
        "\n",
        "solution = 4.5\n",
        "fitness = assess_fitness(solution)\n",
        "print(\"Fitness of the solution:\", fitness)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHF7JvoiULCO",
        "outputId": "eab64689-f072-4e27-db3d-f080396e9960"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Target Function result: -25\n",
            "Fitness of the solution: -20.25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        " What are predictive models, and how do they work? What are descriptive types, and how do you use them? Examples of both types of models should be provided. Distinguish between these two forms of models.\n",
        "\n"
      ],
      "metadata": {
        "id": "7gEXcodpVFY7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Predictive models are used to make predictions or forecasts about future events or outcomes based on available data. These models learn patterns and relationships from historical data to make predictions on new, unseen data. \n",
        "\n",
        "# Example: Predictive models\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "\n",
        "X_train = [[1], [2], [3], [4], [5]]\n",
        "y_train = [2, 4, 6, 8, 10]\n",
        "\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "X_new = [[6]]\n",
        "prediction = model.predict(X_new)\n",
        "print(\"Predicted value:\", prediction)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UC70DdmkUwet",
        "outputId": "afebe829-9186-49c5-a731-ccbf6d70f115"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted value: [12.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Descriptive models are used to describe and summarize data, providing insights and understanding of patterns and relationships within the data. They focus on exploring and explaining the existing data rather than making predictions. \n",
        "\n",
        "# Example: Descriptive Model \n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "\n",
        "X = np.array([[1, 2], [1.5, 1.8], [5, 8], [8, 8], [1, 0.6], [9, 11]])\n",
        "\n",
        "model = KMeans(n_clusters=2)\n",
        "model.fit(X)\n",
        "\n",
        "\n",
        "labels = model.labels_\n",
        "print(\"Cluster labels:\", labels)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2NLVp3hV-1p",
        "outputId": "b0b64689-c344-4873-f321-5bded2ea4284"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cluster labels: [1 1 0 0 1 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Describe the method of assessing a classification model's efficiency in detail. Describe the various measurement parameters."
      ],
      "metadata": {
        "id": "qWXVp2g5a9W4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Accuracy: Proportion of correctly classified instances.\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "y_true = [1,0,1,1,0]\n",
        "y_pred = [1,0,0,1,1]\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "#Precision and Recall: Measures for binary classification tasks.\n",
        "\n",
        "from sklearn.metrics import precision_score , recall_score\n",
        "\n",
        "precision = precision_score(y_true, y_pred)\n",
        "recall = recall_score(y_true, y_pred)\n",
        "\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "\n",
        "\n",
        "#F1 Score: Combines precision and recall into a single value.\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "f1 = f1_score(y_true, y_pred)\n",
        "\n",
        "print(\"F1 score:\", f1)\n",
        "\n",
        "#Confusion Matrix: Tabular representation of predicted labels against true labels\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "#These metrics provide a concise way to assess the efficiency of a classification model, taking into account accuracy, precision, recall, F1 score, and providing a detailed analysis through the confusion matrix.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9GOYW7TavSU",
        "outputId": "699d382f-fd04-430a-b88d-d3c62c7e87ee"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.6\n",
            "Precision: 0.6666666666666666\n",
            "Recall: 0.6666666666666666\n",
            "F1 score: 0.6666666666666666\n",
            "Confusion Matrix:\n",
            "[[1 1]\n",
            " [1 2]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ". \n",
        "      i. In the sense of machine learning models, what is underfitting? What is the most common reason for underfitting?\n",
        "     ii. What does it mean to overfit? When is it going to happen?\n",
        "    iii. In the sense of model fitting, explain the bias-variance trade-off.\n"
      ],
      "metadata": {
        "id": "wqVUUNB4fOIa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1. Underfitting: Underfitting occurs when a model is too simple to capture the patterns in the data. It performs poorly on both the training and test data. The main reason for underfitting is using a model that lacks complexity or flexibility to learn from the data.\n",
        "\n",
        "#2. Overfitting: Overfitting happens when a model learns the training data too well, including noise or random fluctuations. It performs exceptionally well on the training data but fails to generalize to new, unseen data. Overfitting occurs when the model is overly complex or when there is insufficient regularization.\n",
        "\n",
        "#3. Bias-Variance Trade-off: The bias-variance trade-off refers to the relationship between a model's ability to capture true patterns (low bias) and its sensitivity to variations or noise in the data (high variance). A high bias model is too simplistic and may underfit, while a high variance model is overly complex and may overfit. The goal is to strike a balance by finding an optimal level of complexity that minimizes both bias and variance, leading to good generalization on unseen data."
      ],
      "metadata": {
        "id": "FKgEX1gGcpoS"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ": Is it possible to boost the efficiency of a learning model? If so, please clarify how."
      ],
      "metadata": {
        "id": "IoTIvBHJfqpA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Yes, it is possible to boost the efficiency of a learning model. \n",
        "#Increase the amount of training data.\n",
        "#Normalize or scale the input features.\n",
        "#Select the most relevant features.\n",
        "#Tune the model's hyperparameters.\n",
        "#Use cross-validation for robust evaluation.\n",
        "#Apply regularization techniques to prevent overfitting.\n",
        "#Consider ensemble methods for improved performance."
      ],
      "metadata": {
        "id": "UtWLrUO1flet"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ". How would you rate an unsupervised learning model's success? What are the most common success indicators for an unsupervised learning model?\n"
      ],
      "metadata": {
        "id": "uNuyl82YgNZp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Clustering Metrics: Measure the quality of clusters produced by the model, such as Silhouette Score, Calinski-Harabasz Index, or Davies-Bouldin Index.\n",
        "\n",
        "#Visualization: Visualize the clusters or dimensionality-reduced representations to gain insights into the model's performance.\n",
        "\n",
        "#Reconstruction Error: Evaluate how well the model reconstructs the input data, particularly in techniques like autoencoders or PCA.\n",
        "\n",
        "#Domain Expertise: Seek input from domain experts to validate if the discovered patterns or clusters align with prior knowledge or expectations.\n",
        "\n",
        "#Internal Consistency: Assess the intra-cluster similarity and inter-cluster dissimilarity to determine how well the model groups similar instances and separates different instances"
      ],
      "metadata": {
        "id": "zUJR2ZCwgFzg"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Is it possible to use a classification model for numerical data or a regression model for categorical data with a classification model? Explain your answer.\n"
      ],
      "metadata": {
        "id": "UlcBRwmEgdEI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#No, it is not appropriate to use a classification model for numerical data or a regression model for categorical data directly. This is because classification and regression models are designed to handle specific types of data and predict different types of outcomes.\n",
        "\n",
        "#A classification model is trained to predict discrete, categorical labels or classes. It works well when the target variable is categorical, such as predicting whether an email is spam or not, or classifying images into different categories like cats and dogs.\n",
        "\n",
        "#To handle numerical data, regression models such as linear regression, decision trees, or random forests are commonly used. For categorical data, classification models like logistic regression, decision trees, random forests, or support vector machines (SVMs) are more appropriate.\n",
        "\n",
        "#It is important to select the right model type based on the nature of the data and the prediction task to ensure accurate and meaningful results.\n"
      ],
      "metadata": {
        "id": "T_U5lFMXgZtI"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ". Describe the predictive modeling method for numerical values. What distinguishes it from categorical predictive modeling?\n"
      ],
      "metadata": {
        "id": "2om8Q65_gw5P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#The predictive modeling method for numerical values, also known as regression modeling, focuses on predicting continuous numeric outcomes. Here are the key characteristics and distinctions of predictive modeling for numerical values compared to categorical predictive modeling:\n",
        "\n",
        "#Numerical Predictive Modeling:\n",
        "\n",
        "#Outcome variable: Continuous numeric values (e.g., house prices, temperature)\n",
        "#Model output: Numeric value representing the predicted outcome\n",
        "#Evaluation metrics: Mean squared error (MSE), mean absolute error (MAE), R-squared\n",
        "#Model types: Linear regression, decision trees, random forests, SVR, neural networks\n",
        "\n",
        "\n",
        "#Categorical Predictive Modeling:\n",
        "\n",
        "#Outcome variable: Discrete categories or labels (e.g., flower type, customer churn)\n",
        "#Model output: Predicted category or label\n",
        "#Evaluation metrics: Accuracy, precision, recall, F1-score\n",
        "#Model types: Logistic regression, decision trees, random forests, SVMs, neural networks\n"
      ],
      "metadata": {
        "id": "EdZv_J6NgtaJ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " The following data were collected when using a classification model to predict the malignancy of a group of patients' tumors:\n",
        "         i. Accurate estimates – 15 cancerous, 75 benign\n",
        "         ii. Wrong predictions – 3 cancerous, 7 benign\n",
        "                Determine the model's error rate, Kappa value, sensitivity, precision, and F-measure.\n"
      ],
      "metadata": {
        "id": "sQuX_iAaibEB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#error Rate:It represents the overall proportion of incorrect predictions made by the model.\n",
        "\n",
        "#Error Rate = (No. of worng pred)/(Total no. of pred)\n",
        "  =(3+7)/(15+75+3+7)\n",
        "  =10/100\n",
        "  =0.1\n",
        "\n",
        "#kappa value :It measures the agreement between the predicted and actual classes, taking into account the possibility of random agreement.\n",
        "\n",
        "observed Agrement =(no. of accurate cancerous pred + no. of accurate benign pred) / (total no .of pred)\n",
        "=(15+75)/(15+75+3+7)\n",
        "=90/100\n",
        "=0.9\n",
        "\n",
        "#Expected Agreement = (Total number of cancerous predictions) * (Total number of accurate predictions) / (Total number of predictions) +\n",
        "#(Total number of benign predictions) * (Total number of accurate predictions) / (Total number of predictions)\n",
        "\n",
        "Expected Agreement = ((15+3)*(15+75)) / (100)+((75+7)*(15+75))/(100)\n",
        "=(18*90)/100+(82*90)/100\n",
        "=1620/100 + 7380/100\n",
        "=90/100+738/100\n",
        "=828/100\n",
        "=8.28\n",
        "\n",
        "\n",
        "#kappa values:\n",
        "Sensitivity = (Number of accurate cancerous predictions) / (Total number of actual cancerous instances)\n",
        "= 15 / (15 + 3)\n",
        "= 15 / 18\n",
        "= 0.833\n",
        "\n",
        "#Precision: It represents the proportion of positive predictions that are correctly classified.\n",
        "\n",
        "Precision = (Number of accurate cancerous predictions) / (Total number of predicted cancerous instances)\n",
        "= 15 / (15 + 7)\n",
        "= 15 / 22\n",
        "= 0.682\n",
        "\n",
        "#F-measure: It combines precision and recall into a single metric that considers both metrics.\n",
        "F-measure = 2 * (Precision * Sensitivity) / (Precision + Sensitivity)\n",
        "= 2 * (0.682 * 0.833) / (0.682 + 0.833)\n",
        "= 2 * 0.567 / 1.515\n",
        "= 1.134 / 1.515\n",
        "= 0."
      ],
      "metadata": {
        "id": "cQ1f5p0mhJsw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Make quick notes on:\n",
        "         1. The process of holding out\n",
        "         2. Cross-validation by tenfold\n",
        "         3. Adjusting the parameters\n"
      ],
      "metadata": {
        "id": "ALyrRuuukuHi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#The process of holding out: Use train_test_split() function from scikit-learn library to split data into training and validation/test sets.\n",
        "\n",
        "#Cross-validation by tenfold: Use cross_val_score() function from scikit-learn library to perform tenfold cross-validation.\n",
        "\n",
        "#Adjusting the parameters: Use GridSearchCV or RandomizedSearchCV from scikit-learn to find the best combination of parameters.\n"
      ],
      "metadata": {
        "id": "04gdJOXkkyV4"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Define the following terms: \n",
        "         1. Purity vs. Silhouette width\n",
        "         2. Boosting vs. Bagging\n",
        "         3. The eager learner vs. the lazy learner\n",
        "\n"
      ],
      "metadata": {
        "id": "dl4CnuXTlSZt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Purity vs. Silhouette width:\n",
        "\n",
        "Purity: Measure of cluster homogeneity, calculates the proportion of instances in a cluster belonging to the most frequent class.\n",
        "Silhouette width: Measure of cluster separation, calculates the average distance between an instance and its cluster compared to the average distance to instances in the nearest neighboring cluster.\n",
        "\n",
        "#Boosting vs. Bagging:\n",
        "\n",
        "Boosting: Sequentially trains weak learners, focusing on misclassified instances from previous learners to improve overall model performance.\n",
        "Bagging: Trains multiple weak learners independently on random subsets of the training data (with replacement), and aggregates their predictions to reduce variance and improve stability.\n",
        "\n",
        "#Eager learner vs. Lazy learner:\n",
        "\n",
        "Eager learner: Builds a generalization model during training phase using the entire dataset, and uses it for predictions. Examples: decision trees, neural networks, support vector machines.\n",
        "Lazy learner: Defers decision-making until prediction time, memorizing training instances and making predictions based on similarity to stored instances. Examples: k-nearest neighbors (KNN), instance-based learning algorithms."
      ],
      "metadata": {
        "id": "B7HuVGD0lO31"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}